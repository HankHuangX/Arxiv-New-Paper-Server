<html lang="en">

<head>
    <title>Arxiv for Speech Separation</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--===============================================================================================-->
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css"
        href="./vendor/bootstrap/css/bootstrap.min.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css"
        href="./fonts/font-awesome.min.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css" href="./vendor/animate/animate.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css"
        href=".vendor/select2/select2.min.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css"
        href="./vendor/perfect-scrollbar/perfect-scrollbar.css">
    <!--===============================================================================================-->
    <link rel="stylesheet" type="text/css" href="./static/css/util.css">
    <link rel="stylesheet" type="text/css" href="./static/css/main.css">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!--===============================================================================================-->
</head>

<body>
    <div class="limiter">
        <div class="container-table100">
            <div class="wrap-table100">
                <div class="table100">
                    <div class="container pb-3" style="max-width: 1570px;">
                        <p><kbd>Arxiv Speech Separation - Tue Apr 28 23:40:24 2020</kbd></p>
                        <p><kbd>&copy 2020 Copyright: <a href="www.likai.show">Kai Li</a></kbd></p>
                        <p><kbd><span id="busuanzi_value_page_pv"></span> visit this page</kbd></p>
                    </div>
                    <table style="width: 100%">
                        <colgroup>
                            <col span="1" style="width: 35%;">
                            <col span="1" style="width: 15%;">
                            <col span="1" style="width: 45%;">
                            <col span="1" style="width: 15%;">
                        </colgroup>
                        <thead>
                            <tr class="table100-head">
                                <th class="text-center">Title</th>
                                <th class="text-center">Author</th>
                                <th class="text-center">Abstract</th>
                                <th class="text-center">Time</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">CHiME-6 Challenge:Tackling Multispeaker Speech Recognition for Unsegmented Recordings                    </a>                </td>                 <td class="column3 text-center">Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent</td>                <td class="column3 text-justify">Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.</td>                <td class="column3 text-center">20 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Simultaneous Denoising and Dereverberation Using Deep Embedding Features                    </a>                </td>                 <td class="column3 text-center">Cunhang Fan, Jianhua Tao, Bin Liu, Jiangyan Yi, Zhengqi Wen</td>                <td class="column3 text-justify">DC is a state-of-the-art method for speech separation that includes embedding learning and K-means clustering</td>                <td class="column3 text-center">6 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Deep Attention Fusion Feature for Speech Separation with End-to-End Post-filter Method                    </a>                </td>                 <td class="column3 text-center">Cunhang Fan, Jianhua Tao, Bin Liu, Jiangyan Yi, Zhengqi Wen, Xuefei Liu</td>                <td class="column3 text-justify">Experimental results on the WSJ0-2mix dataset show that the proposed method outperforms the state-of-the-art speech separation method. Compared with the pre-separation method, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative improvements in scale-invariant source-to-noise ratio (SI-SNR), the signal-to-distortion ratio (SDR), the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligibility (STOI) measures, respectively.</td>                <td class="column3 text-center">17 March, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Enhancing End-to-End Multi-channel Speech Separation via Spatial Feature Learning                    </a>                </td>                 <td class="column3 text-center">Rongzhi Gu, Shi-Xiong Zhang, Lianwu Chen, Yong Xu, Meng Yu, Dan Su, Yuexian Zou, Dong Yu</td>                <td class="column3 text-justify">These filters are implemented by a 2d convolution (conv2d) layer and their parameters are optimized using a speech separation objective function in a purely data-driven fashion. Evaluation results on simulated multi-channel reverberant WSJ0 2-mix dataset demonstrate that our proposed ICD based MCSS model improves the overall signal-to-distortion ratio by 10.4% over the IPD based MCSS model.</td>                <td class="column3 text-center">13 March, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Wavesplit: End-to-End Speech Separation by Speaker Clustering                    </a>                </td>                 <td class="column3 text-center">Neil Zeghidour, David Grangier</td>                <td class="column3 text-justify">Our model infers a set of speaker representations through clustering, which addresses the fundamental permutation problem of speech separation. We show that Wavesplit outperforms the previous state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2mix, WSJ0-3mix), as well as in noisy (WHAM!) and reverberated (WHAMR!) conditions</td>                <td class="column3 text-center">20 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">La Furca: Iterative Context-Aware End-to-End Monaural Speech Separation Based on Dual-Path Deep Parallel Inter-Intra Bi-LSTM with Attention                    </a>                </td>                 <td class="column3 text-center">Ziqiang Shi, Rujie Liu, Jiqing Han</td>                <td class="column3 text-justify">In this paper, we propose several improvements of dual-path BiLSTM based network for end-to-end approach to monaural speech separation, which consists of 1) dual-path network with intra-parallel BiLSTM and inter-parallel BiLSTM components, 2) global context aware inter-intra cross-parallel BiLSTM, 3) local context-aware network with attention BiLSTM, 4) multiple spiral iterative refinement dual-path BiLSTM (this method is also called PitchFork), that all these networks take the mixed utterance of two speakers and map it to two separated utterances, where each utterance contains only one speaker's voice. Our experiments on the public WSJ0-2mix data corpus results in 19.86dB SDR improvement, 3.63 of PESQ, and 94.2\% of ESTOI, which shows our proposed networks can lead to performance improvement on the speaker separation task</td>                <td class="column3 text-center">26 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Improving speaker discrimination of target speech extraction with time-domain SpeakerBeam                    </a>                </td>                 <td class="column3 text-center">Marc Delcroix, Tsubasa Ochiai, Katerina Zmolikova, Keisuke Kinoshita, Naohiro Tawara, Tomohiro Nakatani, Shoko Araki</td>                <td class="column3 text-justify">First, we propose a time-domain implementation of SpeakerBeam similar to that proposed for a time-domain audio separation network (TasNet), which has achieved state-of-the-art performance for speech separation</td>                <td class="column3 text-center">23 January, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Audio-visual Recognition of Overlapped speech for the LRS2 dataset                    </a>                </td>                 <td class="column3 text-center">Jianwei Yu, Shi-Xiong Zhang, Jian Wu, Shahram Ghorbani, Bo Wu, Shiyin Kang, Shansong Liu, Xunying Liu, Helen Meng, Dong Yu</td>                <td class="column3 text-justify">Third, in contrast to a traditional pipelined architecture containing explicit speech separation and recognition components, a streamlined and integrated AVSR system optimized consistently using the lattice-free MMI (LF-MMI) discriminative criterion is also proposed. Experiments on overlapped speech simulated from the LRS2 dataset suggest the proposed AVSR system outperformed the audio only baseline LF-MMI DNN system by up to 29.98\% absolute in word error rate (WER) reduction, and produced recognition performance comparable to a more complex pipelined system. Consistent performance improvements of 4.89\% absolute in WER reduction over the baseline AVSR system using feature fusion are also obtained.</td>                <td class="column3 text-center">6 January, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Utterance-level Permutation Invariant Training with Latency-controlled BLSTM for Single-channel Multi-talker Speech Separation                    </a>                </td>                 <td class="column3 text-center">Lu Huang, Gaofeng Cheng, Pengyuan Zhang, Yi Yang, Shumin Xu, Jiasong Sun</td>                <td class="column3 text-justify">Evaluated on the WSJ0 two-talker mixed-speech separation task, the absolute gap of signal-to-distortion ratio (SDR) between uPIT-BLSTM and uPIT-LC-BLSTM is reduced to within 0.7 dB.</td>                <td class="column3 text-center">25 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">End-to-end training of time domain audio separation and recognition                    </a>                </td>                 <td class="column3 text-center">Thilo von Neumann, Keisuke Kinoshita, Lukas Drude, Christoph Boeddeker, Marc Delcroix, Tomohiro Nakatani, Reinhold Haeb-Umbach</td>                <td class="column3 text-justify">The rising interest in single-channel multi-speaker speech separation sparked development of End-to-End (E2E) approaches to multi-speaker speech recognition. Our experiments show a word error rate of 11.0% on WSJ0-2mix and indicate that our joint time domain model can yield substantial improvements over cascade DNN-HMM and monolithic E2E frequency domain systems proposed so far.</td>                <td class="column3 text-center">13 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Advances in Online Audio-Visual Meeting Transcription                    </a>                </td>                 <td class="column3 text-center">Takuya Yoshioka, Igor Abramovski, Cem Aksoylar, Zhuo Chen, Moshe David, Dimitrios Dimitriadis, Yifan Gong, Ilya Gurvich, Xuedong Huang, Yan Huang, Aviv Hurvitz, Li Jiang, Sharon Koubi, Eyal Krupka, Ido Leichter, Changliang Liu, Partha Parthasarathy, Alon Vinnikov, Lingfeng Wu, Xiong Xiao, Wayne Xiong, Huaming Wang, Zhenghao Wang, Jun Zhang, Yong Zhao</td>                <td class="column3 text-justify">We show that this problem can be addressed by using a continuous speech separation approach. Experimental results using recordings of natural meetings involving up to 11 attendees are reported. The continuous speech separation improves a word error rate (WER) by 16.1% compared with a highly tuned beamformer. When a complete list of meeting attendees is available, the discrepancy between WER and speaker-attributed WER is only 1.0%, indicating accurate word-to-speaker association. This increases marginally to 1.6% when 50% of the attendees are unknown to the system.</td>                <td class="column3 text-center">10 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Audio-Visual Target Speaker Extraction on Multi-Talker Environment using Event-Driven Cameras                    </a>                </td>                 <td class="column3 text-center">Ander Arriandiaga, Giovanni Morrone, Luca Pasa, Leonardo Badino, Chiara Bartolozzi</td>                <td class="column3 text-justify">All audio-visual speech separation approaches use a frame-based video to extract visual features. However, these frame-based cameras usually work at 30 frames per second</td>                <td class="column3 text-center">5 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Mixup-breakdown: a consistency training method for improving generalization of speech separation models                    </a>                </td>                 <td class="column3 text-center">Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu</td>                <td class="column3 text-justify">Deep-learning based speech separation models confront poor generalization problem that even the state-of-the-art models could abruptly fail when evaluating them in mismatch conditions. The result indicates that MBT significantly outperforms several strong baselines with up to 13.77% relative SI-SNRi improvement</td>                <td class="column3 text-center">3 March, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">MIMO-SPEECH: End-to-End Multi-Channel Multi-Speaker Speech Recognition                    </a>                </td>                 <td class="column3 text-center">Xuankai Chang, Wangyou Zhang, Yanmin Qian, Jonathan Le Roux, Shinji Watanabe</td>                <td class="column3 text-justify">In this work, we propose a novel neural sequence-to-sequence (seq2seq) architecture, MIMO-Speech, which extends the original seq2seq to deal with multi-channel input and multi-channel output so that it can fully model multi-channel multi-speaker speech separation and recognition. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model. The experiments on the spatialized wsj1-2mix corpus show that our model can achieve more than 60% WER reduction compared to the single-channel system with high quality enhanced signals (SI-SDR = 23.1 dB) obtained by the above separation function.</td>                <td class="column3 text-center">15 October, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Dual-path RNN: efficient long sequence modeling for time-domain single-channel speech separation                    </a>                </td>                 <td class="column3 text-center">Yi Luo, Zhuo Chen, Takuya Yoshioka</td>                <td class="column3 text-justify">Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional time-frequency-based methods. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system.</td>                <td class="column3 text-center">27 March, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Audio-Visual Speech Separation and Dereverberation with a Two-Stage Multimodal Network                    </a>                </td>                 <td class="column3 text-center">Ke Tan, Yong Xu, Shi-Xiong Zhang, Meng Yu, Dong Yu</td>                <td class="column3 text-justify">In this study, we address joint speech separation and dereverberation, which aims to separate target speech from background noise, interfering speech and room reverberation. We find that our network achieves a 21.10% improvement in ESTOI and a 0.79 improvement in PESQ over the unprocessed mixtures</td>                <td class="column3 text-center">10 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">A comprehensive study of speech separation: spectrogram vs waveform separation                    </a>                </td>                 <td class="column3 text-center">Fahimeh Bahmaninezhad, Jian Wu, Rongzhi Gu, Shi-Xiong Zhang, Yong Xu, Meng Yu, Dong Yu</td>                <td class="column3 text-justify">We study the speech separation problem for far-field data (more similar to naturalistic audio streams) and develop multi-channel solutions for both frequency and time-domain separators with utilizing spectral, spatial and speaker location information. Multi-channel framework as well is shown to improve the single-channel performance relatively up to +35.5% and +46% in terms of WER and SDR, respectively.</td>                <td class="column3 text-center">23 July, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">End-to-End Multi-Channel Speech Separation                    </a>                </td>                 <td class="column3 text-center">Rongzhi Gu, Jian Wu, Shi-Xiong Zhang, Lianwu Chen, Yong Xu, Meng Yu, Dan Su, Yuexian Zou, Dong Yu</td>                <td class="column3 text-justify">This paper extended the previous approach and proposed a new end-to-end model for multi-channel speech separation. The primary contributions of this work include 1) an integrated waveform-in waveform-out separation system in a single neural network architecture</td>                <td class="column3 text-center">27 May, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Universal Sound Separation                    </a>                </td>                 <td class="column3 text-center">Ilya Kavalerov, Scott Wisdom, Hakan Erdogan, Brian Patton, Kevin Wilson, Jonathan Le Roux, John R. Hershey</td>                <td class="column3 text-justify">In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.</td>                <td class="column3 text-center">2 August, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Improved Speaker-Dependent Separation for CHiME-5 Challenge                    </a>                </td>                 <td class="column3 text-center">Jian Wu, Yong Xu, Shi-Xiong Zhang, Lian-Wu Chen, Meng Yu, Lei Xie, Dong Yu</td>                <td class="column3 text-justify">We adopt a speaker-aware training method by using i-vector as the target speaker information for multi-talker speech separation. With only one unified separation model for all speakers, we achieve a 10\% absolute improvement in terms of word error rate (WER) over the previous baseline of 80.28\% on the development set by leveraging our newly proposed data processing techniques and beamforming approach. With our improved back-end acoustic model, we further reduce WER to 60.15\% which surpasses the result of our submitted CHiME-5 challenge system without applying any fusion techniques.</td>                <td class="column3 text-center">7 April, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Low-Latency Deep Clustering For Speech Separation                    </a>                </td>                 <td class="column3 text-center">Shanshan Wang, Gaurav Naithani, Tuomas Virtanen</td>                <td class="column3 text-justify">This paper proposes a low algorithmic latency adaptation of the deep clustering approach to speaker-independent speech separation. It consists of three parts: a) the usage of long-short-term-memory (LSTM) networks instead of their bidirectional variant used in the original work, b) using a short synthesis window (here 8 ms) required for low-latency operation, and, c) using a buffer in the beginning of audio mixture to estimate cluster centres corresponding to constituent speakers which are then utilized to separate speakers within the rest of the signal. The buffer duration would serve as an initialization phase after which the system is capable of operating with 8 ms algorithmic latency. Moreover, using an 8 ms synthesis window instead of 32 ms degrades the separation performance by around 2.1 dB as compared to the baseline</td>                <td class="column3 text-center">19 February, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">FurcaNeXt: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks                    </a>                </td>                 <td class="column3 text-center">Ziqiang Shi, Huibin Lin, Liu Liu, Rujie Liu, Jiqing Han, Anyan Shi</td>                <td class="column3 text-justify">In this paper we propose several improvements of TCN for end-to-end approach to monaural speech separation, which consists of 1) multi-scale dynamic weighted gated dilated convolutional pyramids network (FurcaPy), 2) gated TCN with intra-parallel convolutional components (FurcaPa), 3) weight-shared multi-scale gated TCN (FurcaSh), 4) dilated TCN with gated difference-convolutional component (FurcaSu), that all these networks take the mixed utterance of two speakers and maps it to two separated utterances, where each utterance contains only one speaker's voice</td>                <td class="column3 text-center">18 April, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Speech Separation Using Gain-Adapted Factorial Hidden Markov Models                    </a>                </td>                 <td class="column3 text-center">Martin H. Radfar, Richard M. Dansereau, Willy Wong</td>                <td class="column3 text-justify">We present a new probabilistic graphical model which generalizes factorial hidden Markov models (FHMM) for the problem of single-channel speech separation (SCSS) in which we wish to separate the two speech signals $X(t)$ and $V(t)$ from a single recording of their mixture $Y(t)=X(t)+V(t)$ using the trained models of the speakers' speech signals. Experimental results, conducted on 180 mixtures with gain differences from 0 to 15~dB, show that the proposed technique significantly outperforms FHMM and its memoryless counterpart, i.e., vector quantization (VQ)-based SCSS.</td>                <td class="column3 text-center">22 January, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Building Corpora for Single-Channel Speech Separation Across Multiple Domains                    </a>                </td>                 <td class="column3 text-center">Matthew Maciejewski, Gregory Sell, Leibny Paola Garcia-Perera, Shinji Watanabe, Sanjeev Khudanpur</td>                <td class="column3 text-justify">To date, the bulk of research on single-channel speech separation has been conducted using clean, near-field, read speech, which is not representative of many modern applications. We produced datasets that are more representative of realistic applications using the CHiME-5 and Mixer 6 corpora and evaluate standard methods on this data to demonstrate the shortcomings of current source-separation performance</td>                <td class="column3 text-center">6 November, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Investigation of Monaural Front-End Processing for Robust ASR without Retraining or Joint-Training                    </a>                </td>                 <td class="column3 text-center">Zhihao Du, Xueliang Zhang, Jiqing Han</td>                <td class="column3 text-justify">In recent years, monaural speech separation has been formulated as a supervised learning problem, which has been systematically researched and shown the dramatical improvement of speech intelligibility and quality for human listeners. We find that directly feeding the enhanced features to ASR can make 36.40% and 11.78% relative WER reduction for the GMM-based and DNN-based ASR respectively</td>                <td class="column3 text-center">24 October, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Recognizing Overlapped Speech in Meetings: A Multichannel Separation Approach Using Neural Networks                    </a>                </td>                 <td class="column3 text-center">Takuya Yoshioka, Hakan Erdogan, Zhuo Chen, Xiong Xiao, Fil Alleva</td>                <td class="column3 text-justify">While speech overlaps have been regarded as a major obstacle in accurately transcribing meetings, a traditional beamformer with a single output has been exclusively used because previously proposed speech separation techniques have critical constraints for application to real meetings. Our meeting transcription system using the unmixing transducer outperforms a system based on a state-of-the-art neural mask-based beamformer by 10.8%</td>                <td class="column3 text-center">8 October, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Single-Microphone Speech Enhancement and Separation Using Deep Learning                    </a>                </td>                 <td class="column3 text-center">Morten KolbÃ¦k</td>                <td class="column3 text-justify">Furthermore, we propose uPIT, a deep learning-based algorithm for single-microphone speech separation and we report state-of-the-art results on a speaker-independent multi-talker speech separation task</td>                <td class="column3 text-center">4 December, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Deep neural network based speech separation optimizing an objective estimator of intelligibility for low latency applications                    </a>                </td>                 <td class="column3 text-center">Gaurav Naithani, Joonas Nikunen, Lars BramslÃ¸w, Tuomas Virtanen</td>                <td class="column3 text-justify">Mean square error (MSE) has been the preferred choice as loss function in the current deep neural network (DNN) based speech separation techniques. We focus on applications where low algorithmic latency ($\leq 10$ ms) is important</td>                <td class="column3 text-center">18 July, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction                    </a>                </td>                 <td class="column3 text-center">Zhong-Qiu Wang, Jonathan Le Roux, DeLiang Wang, John R. Hershey</td>                <td class="column3 text-justify">This paper proposes an end-to-end approach for single-channel speaker-independent multi-speaker speech separation, where time-frequency (T-F) masking, the short-time Fourier transform (STFT), and its inverse are represented as layers within a deep network. On the publicly-available wsj0-2mix dataset, our approach achieves state-of-the-art 12.6 dB scale-invariant signal-to-distortion ratio (SI-SDR) and 13.1 dB SDR, revealing new possibilities for deep learning based phase reconstruction and representing a fundamental progress towards solving the notoriously-hard cocktail party problem.</td>                <td class="column3 text-center">26 April, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation                    </a>                </td>                 <td class="column3 text-center">Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, Michael Rubinstein</td>                <td class="column3 text-justify">Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech</td>                <td class="column3 text-center">9 August, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Cracking the cocktail party problem by multi-beam deep attractor network                    </a>                </td>                 <td class="column3 text-center">Zhuo Chen, Jinyu Li, Xiong Xiao, Takuya Yoshioka, Huaming Wang, Zhenghao Wang, Yifan Gong</td>                <td class="column3 text-justify">For this beamforming, we propose to use differential beamformers as they are more suitable for speech separation. To evaluate the proposed system, we create a challenging dataset comprising mixtures of 2, 3 or 4 speakers. Our results show that the proposed system largely improves the state of the art in speech separation, achieving 11.5 dB, 11.76 dB and 11.02 dB average signal-to-distortion ratio improvement for 4, 3 and 2 overlapped speaker mixtures, which is comparable to the performance of a minimum variance distortionless response beamformer that uses oracle location, source, and noise information. We also run speech recognition with a clean trained acoustic model on the separated speech, achieving relative word error rate (WER) reduction of 45.76\%, 59.40\% and 62.80\% on fully overlapped speech of 4, 3 and 2 speakers, respectively</td>                <td class="column3 text-center">29 March, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">TasNet: time-domain audio separation network for real-time, single-channel speech separation                    </a>                </td>                 <td class="column3 text-center">Yi Luo, Nima Mesgarani</td>                <td class="column3 text-justify">Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output</td>                <td class="column3 text-center">17 April, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Two-stage model and optimal SI-SNR for monaural multi-speaker speech separation in noisy environment                    </a>                </td>                 <td class="column3 text-center">Chao Ma, Dongmei Li, Xupeng Jia</td>                <td class="column3 text-justify">With the developing of deep learning approaches, much progress has been performed on monaural multi-speaker speech separation</td>                <td class="column3 text-center">14 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">WaveCRN: An Efficient Convolutional Recurrent Neural Network for End-to-end Speech Enhancement                    </a>                </td>                 <td class="column3 text-center">Tsun-An Hsieh, Hsin-Min Wang, Xugang Lu, Yu Tsao</td>                <td class="column3 text-justify">In addition, in order to more effectively suppress the noise components in the input noisy speech, we derive a novel restricted feature masking (RFM) approach that performs enhancement on the feature maps in the hidden layers; this is different from the approach that applies the estimated ratio mask on the noisy spectral features, which is commonly used in speech separation methods. Experimental results on speech denoising and compressed speech restoration tasks confirm that with the lightweight architecture of SRU and the feature-mapping-based RFM, WaveCRN performs comparably with other state-of-the-art approaches with notably reduced model complexity and inference time.</td>                <td class="column3 text-center">12 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Multi-modal Multi-channel Target Speech Separation                    </a>                </td>                 <td class="column3 text-center">Rongzhi Gu, Shi-Xiong Zhang, Yong Xu, Lianwu Chen, Yuexian Zou, Dong Yu</td>                <td class="column3 text-justify">This work proposes a general multi-modal framework for target speech separation by utilizing all the available information of the target speaker, including his/her spatial location, voice characteristics and lip movements. Experiment results illustrate that our proposed multi-modal framework significantly outperforms single-modal and bi-modal speech separation approaches, while can still support real-time processing.</td>                <td class="column3 text-center">16 March, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Real-time binaural speech separation with preserved spatial cues                    </a>                </td>                 <td class="column3 text-center">Cong Han, Yi Luo, Nima Mesgarani</td>                <td class="column3 text-justify">Here, we propose a speech separation algorithm that preserves the interaural cues of separated sound sources and can be implemented with low latency and high fidelity, therefore enabling a real-time modification of the acoustic scene. Based on the time-domain audio separation network (TasNet), a single-channel time-domain speech separation system that can be implemented in real-time, we propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that takes binaural mixed audio as input and simultaneously separates target speakers in both channels</td>                <td class="column3 text-center">16 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Spatial and spectral deep attention fusion for multi-channel speech separation using deep embedding features                    </a>                </td>                 <td class="column3 text-center">Cunhang Fan, Bin Liu, Jianhua Tao, Jiangyan Yi, Zhengqi Wen</td>                <td class="column3 text-justify">Multi-channel deep clustering (MDC) has acquired a good performance for speech separation</td>                <td class="column3 text-center">4 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Continuous speech separation: dataset and analysis                    </a>                </td>                 <td class="column3 text-center">Zhuo Chen, Takuya Yoshioka, Liang Lu, Tianyan Zhou, Zhong Meng, Yi Luo, Jian Wu, Xiong Xiao, Jinyu Li</td>                <td class="column3 text-justify">Most prior studies on speech separation use pre-segmented signals of artificially mixed speech utterances which are mostly \emph{fully} overlapped, and the algorithms are evaluated based on signal-to-distortion ratio or similar performance metrics. In this paper, we define continuous speech separation (CSS) as a task of generating a set of non-overlapped speech signals from a \textit{continuous} audio stream that contains multiple utterances that are \emph{partially} overlapped by a varying degree</td>                <td class="column3 text-center">10 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Temporal-Spatial Neural Filter: Direction Informed End-to-End Multi-channel Target Speech Separation                    </a>                </td>                 <td class="column3 text-center">Rongzhi Gu, Yuexian Zou</td>                <td class="column3 text-justify">Despite the recent advances in deep learning based close-talk speech separation, the applications to real-world are still an open issue. Experimental results demonstrate that the proposed method outperforms state-of-the-art deep learning based multi-channel approaches with fewer parameters and faster processing speed</td>                <td class="column3 text-center">2 January, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">A Unified Framework for Speech Separation                    </a>                </td>                 <td class="column3 text-center">Fahimeh Bahmaninezhad, Shi-Xiong Zhang, Yong Xu, Meng Yu, John H. L. Hansen, Dong Yu</td>                <td class="column3 text-justify">The initial solutions introduced for deep learning based speech separation analyzed the speech signals into time-frequency domain with STFT; and then encoded mixed signals were fed into a deep neural network based separator. We extend single-channel speech separation into multi-channel framework with end-to-end training of the network while optimizing the speech separation criterion (i.e., Si-SNR) directly</td>                <td class="column3 text-center">16 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">MITAS: A Compressed Time-Domain Audio Separation Network with Parameter Sharing                    </a>                </td>                 <td class="column3 text-center">Chao-I Tuan, Yuan-Kuei Wu, Hung-yi Lee, Yu Tsao</td>                <td class="column3 text-justify">Deep learning methods have brought substantial advancements in speech separation (SS). They achieved state-of-the-art results on several standardized SS tasks</td>                <td class="column3 text-center">9 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Improving Voice Separation by Incorporating End-to-end Speech Recognition                    </a>                </td>                 <td class="column3 text-center">Naoya Takahashi, Mayank Kumar Singh, Sakya Basak, Parthasaarathy Sudarsanam, Sriram Ganapathy, Yuki Mitsufuji</td>                <td class="column3 text-justify">Experimental results on speech separation and enhancement task on the AVSpeech dataset show that the proposed method significantly improves the signal-to-distortion ratio over the baseline model and even outperforms an audio visual model, that utilizes visual information of lip movements.</td>                <td class="column3 text-center">28 November, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Demystifying TasNet: A Dissecting Approach                    </a>                </td>                 <td class="column3 text-center">Jens Heitkaemper, Darius Jakobeit, Christoph Boeddeker, Lukas Drude, Reinhold Haeb-Umbach</td>                <td class="column3 text-justify">In recent years time domain speech separation has excelled over frequency domain separation in single channel scenarios and noise-free environments</td>                <td class="column3 text-center">5 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Online Spectrogram Inversion for Low-Latency Audio Source Separation                    </a>                </td>                 <td class="column3 text-center">Paul Magron, Tuomas Virtanen</td>                <td class="column3 text-justify">Experiments conducted on a speech separation task show that oMISI performs as well as its offline counterpart, thus demonstrating its potential for real-time source separation.</td>                <td class="column3 text-center">24 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Onssen: an open-source speech separation and enhancement library                    </a>                </td>                 <td class="column3 text-center">Zhaoheng Ni, Michael I Mandel</td>                <td class="column3 text-justify">Recently many deep learning approaches are proposed and have been constantly refreshing the state-of-the-art performances. We introduce "onssen": an open-source speech separation and enhancement library</td>                <td class="column3 text-center">3 November, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">End-to-end Microphone Permutation and Number Invariant Multi-channel Speech Separation                    </a>                </td>                 <td class="column3 text-center">Yi Luo, Zhuo Chen, Nima Mesgarani, Takuya Yoshioka</td>                <td class="column3 text-justify">In this paper, we propose transform-average-concatenate (TAC), a simple design paradigm for channel permutation and number invariant multi-channel speech separation. Moreover, we show that TAC also significantly improves the separation performance with fixed geometry array configuration, further proving the effectiveness of the proposed paradigm in the general problem of multi-microphone speech separation.</td>                <td class="column3 text-center">27 March, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Interrupted and cascaded permutation invariant training for speech separation                    </a>                </td>                 <td class="column3 text-center">Gene-Ping Yang, Szu-Lin Wu, Yao-Wen Mao, Hung-yi Lee, Lin-shan Lee</td>                <td class="column3 text-justify">Permutation Invariant Training (PIT) has long been a stepping stone method for training speech separation model in handling the label ambiguity problem. With fixed label training cascaded between two sections of PIT, we achieved the state-of-the-art performance on WSJ0-2mix without changing the model architecture at all.</td>                <td class="column3 text-center">28 October, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Multi-channel Speech Separation Using Deep Embedding Model with Multilayer Bootstrap Networks                    </a>                </td>                 <td class="column3 text-center">Ziye Yang, Xiao-Lei Zhang</td>                <td class="column3 text-justify">Recently, deep clustering (DPCL) based speaker-independent speech separation has drawn much attention, since it needs little speaker prior information</td>                <td class="column3 text-center">24 October, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Filterbank design for end-to-end speech separation                    </a>                </td>                 <td class="column3 text-center">Manuel Pariente, Samuele Cornell, Antoine Deleforge, Emmanuel Vincent</td>                <td class="column3 text-justify">Single-channel speech separation has recently made great progress thanks to learned filterbanks as used in ConvTasNet. We evaluate these filterbanks on a newly released noisy speech separation dataset (WHAM)</td>                <td class="column3 text-center">28 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">WHAMR!: Noisy and Reverberant Single-Channel Speech Separation                    </a>                </td>                 <td class="column3 text-center">Matthew Maciejewski, Gordon Wichern, Emmett McQuinn, Jonathan Le Roux</td>                <td class="column3 text-justify">The spectral smearing caused by reverberation can result in significant performance degradation for standard deep learning-based speech separation systems, which rely on spectral structure and the sparsity of speech signals to tease apart sources</td>                <td class="column3 text-center">14 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue System                    </a>                </td>                 <td class="column3 text-center">Yun-Wei Chu, Kuan-Yen Lin, Chao-Chun Hsu, Lun-Wei Ku</td>                <td class="column3 text-justify">The 8-th Dialog System Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog (AVSD) task, which contains multiple modalities including audio, vision, and language, to evaluate how dialogue systems understand different modalities and response to users. Compared to the baseline released by AVSD organizers, our model achieves a relative 12.1% and 22.4% improvement over the baseline on ROUGE-L score and CIDEr score.</td>                <td class="column3 text-center">17 January, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Multi-Layer Content Interaction Through Quaternion Product For Visual Question Answering                    </a>                </td>                 <td class="column3 text-center">Lei Shi, Shijie Geng, Kai Shuang, Chiori Hori, Songxiang Liu, Peng Gao, Sen Su</td>                <td class="column3 text-justify">Multi-modality fusion technologies have greatly improved the performance of neural network-based Video Description/Caption, Visual Question Answering (VQA) and Audio Visual Scene-aware Dialog (AVSD) over the recent years. The evaluation results show our QBN improved the performance on VQA 2.0, even though using surpass large scale BERT or visual BERT pre-trained models</td>                <td class="column3 text-center">16 February, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog                    </a>                </td>                 <td class="column3 text-center">Shachi H Kumar, Eda Okur, Saurav Sahay, Jonathan Huang, Lama Nachman</td>                <td class="column3 text-justify">In this work, we present three main architectural explorations for the Audio Visual Scene-Aware Dialog (AVSD): 1) investigating `topics' of the dialog as an important contextual feature for the conversation, 2) exploring several multimodal attention mechanisms during response generation, 3) incorporating an end-to-end audio classification ConvNet, AclNet, into our architecture</td>                <td class="column3 text-center">20 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation                    </a>                </td>                 <td class="column3 text-center">Kuan-Yen Lin, Chao-Chun Hsu, Yun-Nung Chen, Lun-Wei Ku</td>                <td class="column3 text-justify">2018) proposed an Audio Visual Scene-aware Dialog (AVSD) task, which contains five modalities including video, dialogue history, summary, and caption, as a scene-aware environment</td>                <td class="column3 text-center">21 August, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Learning Question-Guided Video Representation for Multi-Turn Video Question Answering                    </a>                </td>                 <td class="column3 text-center">Guan-Lin Chao, Abhinav Rastogi, Semih Yavuz, Dilek Hakkani-TÃ¼r, Jindong Chen, Ian Lane</td>                <td class="column3 text-justify">Through empirical evaluation on the Audio Visual Scene-aware Dialog (AVSD) dataset, our proposed models in single-turn and multi-turn question answering achieve state-of-the-art performance on several automatic natural language generation evaluation metrics.</td>                <td class="column3 text-center">30 July, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">From FiLM to Video: Multi-turn Question Answering with Multi-modal Context                    </a>                </td>                 <td class="column3 text-center">Dat Tien Nguyen, Shikhar Sharma, Hannes Schulz, Layla El Asri</td>                <td class="column3 text-justify">The Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of the Dialog System Technology Challenge 7 (DSTC7), proposes a combined task, where a system has to answer questions pertaining to a video given a dialogue with previous question-answer pairs and the video itself. Compared to the modality-fusing baseline model released by the AVSD challenge organizers, our model achieves a relative improvements of more than 16%, scoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr.</td>                <td class="column3 text-center">17 December, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Depression Scale Recognition from Audio, Visual and Text Analysis                    </a>                </td>                 <td class="column3 text-center">Shubham Dham, Anirudh Sharma, Abhinav Dhall</td>                <td class="column3 text-justify">This report describes work done by us for Audio Visual Emotion Challenge (AVEC) 2017 during our second year BTech summer internship. The results obtained were able to cross the provided baseline on validation data set by 17% on audio features and 24.5% on video features.</td>                <td class="column3 text-center">18 September, 2017</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Development of an Anti-collision Model for Vehicles                    </a>                </td>                 <td class="column3 text-center">A. M. Zungeru</td>                <td class="column3 text-justify">Also incorporated into it is an audio visual alarm to work in with the receiver and effectively alert the driver and/or the passengers. To achieve this design, 555 timers coupled both as astable and monostable circuits were used along with a 38 KHz Square Pulse generator</td>                <td class="column3 text-center">21 December, 2012</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator                    </a>                </td>                 <td class="column3 text-center">Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Kyomin Jung</td>                <td class="column3 text-justify">Audio Visual Scene-aware Dialog (AVSD) is the task of generating a response for a question with a given scene, video, audio, and the history of previous turns in the dialog</td>                <td class="column3 text-center">1 April, 2020</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Exploring Context, Attention and Audio Features for Audio Visual Scene-Aware Dialog                    </a>                </td>                 <td class="column3 text-center">Shachi H Kumar, Eda Okur, Saurav Sahay, Jonathan Huang, Lama Nachman</td>                <td class="column3 text-justify">We develop and test our approaches on the Audio Visual Scene-Aware Dialog (AVSD) dataset released as a part of the DSTC7</td>                <td class="column3 text-center">20 December, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Improving Voice Separation by Incorporating End-to-end Speech Recognition                    </a>                </td>                 <td class="column3 text-center">Naoya Takahashi, Mayank Kumar Singh, Sakya Basak, Parthasaarathy Sudarsanam, Sriram Ganapathy, Yuki Mitsufuji</td>                <td class="column3 text-justify">Experimental results on speech separation and enhancement task on the AVSpeech dataset show that the proposed method significantly improves the signal-to-distortion ratio over the baseline model and even outperforms an audio visual model, that utilizes visual information of lip movements.</td>                <td class="column3 text-center">28 November, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">The Eighth Dialog System Technology Challenge                    </a>                </td>                 <td class="column3 text-center">Seokhwan Kim, Michel Galley, Chulaka Gunasekara, Sungjin Lee, Adam Atkinson, Baolin Peng, Hannes Schulz, Jianfeng Gao, Jinchao Li, Mahmoud Adada, Minlie Huang, Luis Lastras, Jonathan K. Kummerfeld, Walter S. Lasecki, Chiori Hori, Anoop Cherian, Tim K. Marks, Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta</td>                <td class="column3 text-justify">In line with recent challenges, the eighth edition focuses on applying end-to-end dialog technologies in a pragmatic way for multi-domain task-completion, noetic response selection, audio visual scene-aware dialog, and schema-guided dialog state tracking tasks. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks.</td>                <td class="column3 text-center">14 November, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling                    </a>                </td>                 <td class="column3 text-center">Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yu-Hsuan Deng, Shang-Yu Su, Yun-Nung Chen</td>                <td class="column3 text-justify">A more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer. Also, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.</td>                <td class="column3 text-center">14 August, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Log Complex Color for Visual Pattern Recognition of Total Sound                    </a>                </td>                 <td class="column3 text-center">Stephen Wedekind, P. Fraundorf</td>                <td class="column3 text-justify">While traditional audio visualization methods depict amplitude intensities vs</td>                <td class="column3 text-center">23 July, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Audio-Visual Scene-Aware Dialog                    </a>                </td>                 <td class="column3 text-center">Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks, Chiori Hori, Peter Anderson, Stefan Lee, Devi Parikh</td>                <td class="column3 text-justify">To benchmark this task, we introduce the Audio Visual Scene-Aware Dialog (AVSD) Dataset</td>                <td class="column3 text-center">8 May, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Dialog System Technology Challenge 7                    </a>                </td>                 <td class="column3 text-center">Koichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D&#39;Haro, Lazaros Polymenakos, Chulaka Gunasekara, Walter S. Lasecki, Jonathan K. Kummerfeld, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, Xiang Gao, Huda Alamari, Tim K. Marks, Devi Parikh, Dhruv Batra</td>                <td class="column3 text-justify">The seventh DSTC (DSTC7) focuses on developing technologies related to end-to-end dialog systems for (1) sentence selection, (2) sentence generation and (3) audio visual scene aware dialog. Each track introduced new datasets and participants achieved impressive results using state-of-the-art end-to-end technologies.</td>                <td class="column3 text-center">10 January, 2019</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog                    </a>                </td>                 <td class="column3 text-center">Shachi H Kumar, Eda Okur, Saurav Sahay, Juan Jose Alvarado Leanos, Jonathan Huang, Lama Nachman</td>                <td class="column3 text-justify">As a part of the 7th Dialog System Technology Challenges (DSTC7), for Audio Visual Scene-Aware Dialog (AVSD) track, We explore `topics' of the dialog as an important contextual feature into the architecture along with explorations around multimodal Attention</td>                <td class="column3 text-center">20 December, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features                    </a>                </td>                 <td class="column3 text-center">Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh</td>                <td class="column3 text-justify">Using this new dataset for Audio Visual Scene-aware dialog (AVSD), we trained an end-to-end conversation model that generates responses in a dialog about a video</td>                <td class="column3 text-center">29 June, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7                    </a>                </td>                 <td class="column3 text-center">Huda Alamri, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Jue Wang, Irfan Essa, Dhruv Batra, Devi Parikh, Anoop Cherian, Tim K. Marks, Chiori Hori</td>                <td class="column3 text-justify">We introduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset</td>                <td class="column3 text-center">1 June, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Visual Speech Enhancement                    </a>                </td>                 <td class="column3 text-center">Aviv Gabbay, Asaph Shamir, Shmuel Peleg</td>                <td class="column3 text-justify">The proposed model outperforms prior audio visual methods on two public lipreading datasets</td>                <td class="column3 text-center">13 June, 2018</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Multi-Modal Hybrid Deep Neural Network for Speech Enhancement                    </a>                </td>                 <td class="column3 text-center">Zhenzhou Wu, Sunil Sivadas, Yong Kiam Tan, Ma Bin, Rick Siow Mong Goh</td>                <td class="column3 text-justify">In this paper we propose a novel deep learning model inspired by insights from human audio visual perception</td>                <td class="column3 text-center">15 June, 2016</td>            </tr><tr>                <td class="column3 text-center">                    <a href="https://arxiv.org/abs/2004.11339">Fault Modelling in System-of-Systems Contracts                    </a>                </td>                 <td class="column3 text-center">Zoe Andrews, Jeremy Bryans, Richard Payne, Klaus Kristensen</td>                <td class="column3 text-justify">The proposed extensions are introduced with respect to an Audio Visual SoS case study from Bang and Olufsen, before discussing how they relate to previous work on modelling faults in SoSs.</td>                <td class="column3 text-center">7 October, 2014</td>            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>
    <!--===============================================================================================-->
    <script src="./vendor/jquery/jquery-3.2.1.min.js"></script>
    <!--===============================================================================================-->
    <script src="./vendor/bootstrap/js/popper.js"></script>
    <script src="./vendor/bootstrap/js/bootstrap.min.js"></script>
</body>

</html>